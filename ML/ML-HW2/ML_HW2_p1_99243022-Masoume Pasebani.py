# -*- coding: utf-8 -*-
"""ML-HW2-P1-Masoume Pasebani-99243022.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dYOhKV6Ojwpuge7xuZQFEtwzEgLIiBEr
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
from sklearn import metrics
import matplotlib.pyplot as plt
import random

file_id = '117LO2wPC6D-Dr2LfkIVZfZDWrVMoEIGR'
url = f'https://drive.google.com/uc?id={file_id}'

data = pd.read_csv(url)
data

print(data['age'].isna().sum(), data['age'].unique())
print(data['sex'].isna().sum(), data['sex'].unique())
print(data['bmi'].isna().sum(), data['bmi'].unique())
print(data['children'].isna().sum(), data['children'].unique())
print(data['smoker'].isna().sum(), data['smoker'].unique())
print(data['region'].isna().sum(),data['region'].unique())
print(data['charges'].isna().sum(), data['charges'].unique())

for index, row in data.iterrows():
    if row['age'] < 20:
        data.at[index, 'age'] = 'teen'
    if row['age'] >= 20 and row['age'] < 40:
        data.at[index , 'age'] = 'young adults'
    if row['age'] >= 40 and row['age'] <60:
        data.at[index, 'age'] = 'adults'
    if row['age'] >= 60 :
        data.at[index, 'age'] = 'middle age adults'
data['age'].value_counts()

lblenc=LabelEncoder()
zero_one_valued = ['sex','smoker']
for col in zero_one_valued:
  data[col] = lblenc.fit_transform(data[col])
data.head()

tmp = data
enc = OneHotEncoder(sparse_output=False)
age = enc.fit_transform(data[['age']])
enc_data1 = pd.DataFrame(age, columns=enc.get_feature_names_out(['age']))
tmp.drop(columns='age', inplace=True )

region = enc.fit_transform(data[['region']])
enc_data2 = pd.DataFrame(region, columns=enc.get_feature_names_out(['region']))
tmp.drop(columns='region', inplace=True )

OneHotData = tmp.join(enc_data1)
OneHotData = OneHotData.join(enc_data2)
OneHotData

first_column = OneHotData.pop('charges')
OneHotData.insert(12, 'charges', first_column)
OneHotData

d = preprocessing.normalize(OneHotData.iloc[:,0:12])
scaled_df = pd.DataFrame(d, columns=OneHotData.iloc[:,0:12].columns)
scaled_df.insert(12, 'charges', first_column)
scaled_df

X = scaled_df.iloc[:, :-1]
y = scaled_df['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")

def Cost(x , y , w , b):
  predict = np.dot(x,w)+b
  m = X_train.shape[0]
  cost = np.sum((predict - y)**2)/(2*m)
  return cost


def gradient_descent (x , y , w_in , b_in) :
    predict = np.dot(x,w_in)+b_in
    m = X_train.shape[0]
    dj_dw = np.dot((predict-y),x)/m
    dj_db = np.sum(predict-y)/m
    return dj_dw,dj_db

loss = []
w = np.random.randn(X_train.shape[1])
b = np.random.randn()
loss.append(Cost(X_train , y_train , w ,b))

alpha = 0.01
print(w.shape)

tmp = []
for _ in range(10000):
    tmp.append(Cost(X_train, y_train, w, b))
    dw, db = gradient_descent(X_train, y_train, w, b)
    w -= alpha * dw
    b -= alpha * db
loss.append(tmp)

x_train1, x_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.35)
x_train2, x_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.5)
x_train3, x_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.6)
x_train4, x_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.75)
x_train5, x_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.85)

w1 = np.random.randn(x_train1.shape[1])
w2 = np.random.randn(x_train2.shape[1])
w3 = np.random.randn(x_train3.shape[1])
w4 = np.random.randn(x_train4.shape[1])
w5 = np.random.randn(x_train5.shape[1])
cost1=[]
cost2=[]
cost3=[]
cost4=[]
cost5=[]
for _ in range(1000):
  cost1.append(Cost(x_train1 , y_train1 , w1 ,b))
  dw,db = gradient_descent(x_train1 , y_train1 , w1 ,b)
  w1 -= alpha * dw
  b -= alpha * db
  cost2.append(Cost(x_train2 , y_train2 , w2 ,b))
  dw,db = gradient_descent(x_train2 , y_train2 , w2 ,b)
  w2 -= alpha * dw
  b -= alpha * db
  cost3.append(Cost(x_train3 , y_train3 , w3 ,b))
  dw,db = gradient_descent(x_train3 , y_train3 , w3 ,b)
  w3 -= alpha * dw
  b -= alpha * db
  cost4.append(Cost(x_train4 , y_train4 , w4 ,b))
  dw,db = gradient_descent(x_train4 , y_train4 , w4 ,b)
  w4 -= alpha * dw
  b -= alpha * db
  cost5.append(Cost(x_train5 , y_train5 , w5 ,b))
  dw,db = gradient_descent(x_train5 , y_train5 , w5 ,b)
  w5 -= alpha * dw
  b -= alpha * db

plt.plot(range(1000),label = "test size = 0.25")

plt.plot(range(1000), cost1, color = 'g' , label ="test size = 0.35")

plt.plot(range(1000), cost2, color = 'pink' , label = "test size = 0.5")

plt.plot(range(1000), cost3, color = 'yellow' , label ="test size = 0.6")

plt.plot(range(1000), cost4, color = 'r' , label = "test size = 0.75")

plt.plot(range(1000), cost5 , color = 'purple' , label ="test size = 0.85" )

plt.legend()
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)

y_pred = model.predict(X_test)

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, color='blue', alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.title('True vs Predicted Values')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.show()

residuals = y_test - y_pred
plt.figure(figsize=(8,6))
plt.scatter(y_pred, residuals, color='green', alpha=0.6)
plt.axhline(y=0, color='black', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

w = np.random.randn(X_train_scaled.shape[1])
b = np.random.randn()

alpha = 0.01
iterations = 1000
m = len(y_train)
cost_history = []
weights_history = []

plt.figure(figsize=(10, 6))
for i in range(iterations):
    predict = np.dot(X_train_scaled, w) + b

    dj_dw = np.dot(X_train_scaled.T, (predict - y_train.ravel())) / m
    dj_dw = dj_dw.reshape(w.shape)

    dj_db = np.sum(predict - y_train) / m
    w -= alpha * dj_dw
    b -= alpha * dj_db
    cost = Cost(X_train_scaled, y_train, w, b)
    cost_history.append(cost)
    if i % 50 == 0:
        plt.plot(X_train_scaled, np.dot(X_train_scaled, w) + b, label=f"Iteration {i}")

    weights_history.append(w.copy())

plt.scatter(X_train_scaled, y_train, color='blue', alpha=0.6, label="Data points")
plt.title("Regression Line")
plt.xlabel("Feature")
plt.ylabel("Target")
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label="Cost (MSE)")
plt.title("Cost Function Over iterations")
plt.xlabel("iterations")
plt.ylabel("Cost")
plt.legend()
plt.show()

y_pred = np.dot(X_test_scaled, w) + b
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse}")

weights_history = np.array(weights_history)
plt.figure(figsize=(10, 6))
for i in range(weights_history.shape[1]):
    plt.plot(range(iterations), weights_history[:, i], label=f"Weight {i}")
plt.title("Weights changes Over iterations")
plt.xlabel("iterations")
plt.ylabel("W Values")
plt.legend()
plt.show()

print("\nChanges in Weights:")
for i, weight in enumerate(weights_history.T):
    print(f"Weight {i}: {weight[:10]} ... {weight[-10:]}")

from sklearn.linear_model import SGDRegressor


sgd_regressor = SGDRegressor(max_iter=1, tol=None, eta0=0.01, warm_start=True, random_state=42)

cost_history = []
bias_history = []
weights_history = []

for i in range(1000):
    sgd_regressor.fit(X_train_scaled, y_train.ravel())
    weights_history.append(sgd_regressor.coef_.copy())
    bias_history.append(sgd_regressor.intercept_[0])
    y_pred = sgd_regressor.predict(X_train_scaled)
    cost = mean_squared_error(y_train, y_pred) / 2
    cost_history.append(cost)
    if i % 200 == 0:
        print(f"Iteration {i}: Weights = {sgd_regressor.coef_}, Bias = {sgd_regressor.intercept_}, Cost = {cost:.4f}")


plt.figure(figsize=(10, 6))
plt.scatter(X_train_scaled, y_train, color='blue', alpha=0.6, label="Data points")
plt.plot(X_train_scaled, sgd_regressor.predict(X_train_scaled), color='green', label='SGD Regression Line')
plt.title("SGD Regression Line")
plt.xlabel("Feature")
plt.ylabel("Target")
plt.legend()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(range(len(cost_history)), cost_history, label="Cost (MSE)")
plt.title("Cost Function Over iterations (SGD)")
plt.xlabel("iterations")
plt.ylabel("Cost")
plt.legend()
plt.show()

weights_history = np.array(weights_history)
plt.figure(figsize=(10, 6))
for i in range(weights_history.shape[1]):
    plt.plot(range(len(weights_history)), weights_history[:, i], label=f"Weight {i}")
plt.title("Weights changes Over iterations (SGD)")
plt.xlabel("iterations")
plt.ylabel("W Values")
plt.legend()
plt.show()


y_pred_sgd = sgd_regressor.predict(X_test_scaled)
mse_sgd = mean_squared_error(y_test, y_pred_sgd)
print(f"SGD Final MSE: {mse_sgd:.4f}")

print("\nChanges in Weights:")
for i, weight in enumerate(weights_history.T):
    print(f"Weight {i}: {weight[:10]} ... {weight[-10:]}")

