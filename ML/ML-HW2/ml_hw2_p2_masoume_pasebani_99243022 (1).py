# -*- coding: utf-8 -*-
"""ML-HW2-P2-Masoume Pasebani-99243022.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ODmanurQbtiEY4fKHOUr4kQjTphhF3R
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import normalize
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

file_id = '1CwsYiq3UNMAs7iMhHeRjcs6L1eD1EIqU'
url = f'https://drive.google.com/uc?id={file_id}'

data = pd.read_csv(url)
data

print(data.isnull().sum())

data = data.fillna(data.median())

columns = data.columns

for col in columns:
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    print(f'{col}: Lower={lower_bound}, Upper={upper_bound}')

for col in ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
            'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']:
    lower = data[col].quantile(0.25) - 1.5 * (data[col].quantile(0.75) - data[col].quantile(0.25))
    upper = data[col].quantile(0.75) + 1.5 * (data[col].quantile(0.75) - data[col].quantile(0.25))
    outliers = data[(data[col] < lower) | (data[col] > upper)]
    print(f"{col}: {len(outliers)} outliers")

for col in ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',
            'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']:
    lower = data[col].quantile(0.25) - 1.5 * (data[col].quantile(0.75) - data[col].quantile(0.25))
    upper = data[col].quantile(0.75) + 1.5 * (data[col].quantile(0.75) - data[col].quantile(0.25))
    data[col] = np.where(data[col] < lower, lower, np.where(data[col] > upper, upper, data[col]))

features = data.drop(columns=['quality'])
target = data['quality']

normalized_features = normalize(features)
normalized_df = pd.DataFrame(normalized_features, columns=features.columns)

normalized_df['quality'] = target

print(normalized_df.head())

X = data.drop(columns=['quality'])
y = pd.get_dummies(data['quality'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)

def CrossEntropyLoss(y,y_hat):
    L_sum = np.sum(np.multiply(y, np.log(y_hat)))
    m = y.shape[1]
    L = -(1/m) * L_sum

    return L

class LogisticRegression:
    def __init__(self, n_class, lr, epochs, threshold=0.5):
        self.n_class = n_class
        self.lr = lr
        self.epochs = epochs
        self.threshold = threshold
        self.w = None

    def softmax(self, scores):
        max_score = np.max(scores, axis=1, keepdims=True)
        exp_scores = np.exp(scores - max_score)
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)


    def train(self, X_train, y_train):
      self.w = np.zeros((self.n_class, X_train.shape[1]))
      accuracies = []
      for epoch in range(self.epochs):
        scores = np.dot(X_train, self.w.T)
        y_hat = self.softmax(scores)

        loss = CrossEntropyLoss(y_train.values, y_hat)

        gradient = np.dot((y_hat - y_train.values).T, X_train) / X_train.shape[0]
        self.w -= self.lr * gradient

        y_pred_train = self.predict(X_train)

        accuracy = accuracy_score(np.argmax(y_train.values, axis=1), y_pred_train) * 100

        accuracies.append(float(accuracy))

        if epoch % 10 == 0:
            print(f"Epoch {epoch+1}/{self.epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.2f}%")

      return accuracies



    def predict(self, X_test):
        scores = np.dot(X_test, self.w.T)
        y_hat = self.softmax(scores)
        y_pred = np.argmax(y_hat, axis=1)
        return y_pred

    def evaluate(self, X_test, y_test):
        y_pred = self.predict(X_test)

        y_test_labels = np.argmax(y_test.values, axis=1)

        accuracy = accuracy_score(y_test_labels, y_pred)
        f1 = f1_score(y_test_labels, y_pred, average='weighted')
        conf_matrix = confusion_matrix(y_test_labels, y_pred)

        print(f'Accuracy: {accuracy * 100:.2f}%')
        print(f'F1 Score: {f1:.2f}')
        print('Confusion Matrix:')
        print(conf_matrix)

        return accuracy, f1, conf_matrix

    def plot_accuracy_vs_epoch(self, X_train, y_train):
        accuracies = self.train(X_train, y_train)
        plt.plot(range(1, self.epochs + 1), accuracies, marker='o')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Accuracy vs Epochs')
        plt.grid(True)
        plt.show()

def visualize_predictions(y_true, y_pred):
    plt.figure(figsize=(10, 6))
    plt.plot(y_true[:50], label='True Labels', marker='o')
    plt.plot(y_pred[:50], label='Predicted Labels', marker='x')
    plt.xlabel('Sample Index')
    plt.ylabel('Class Label')
    plt.title('True vs Predicted Labels')
    plt.legend()
    plt.show()

model = LogisticRegression(n_class=y_train.shape[1], lr=0.01, epochs=100)

model.train(X_train, y_train)

y_test_labels = np.argmax(y_test.values, axis=1)
visualize_predictions(y_test_labels, model.predict(X_test))

class LogisticRegression2(LogisticRegression):
    def __init__(self, n_class, lr, epochs, lambda_reg=0.01):
        super().__init__(n_class, lr, epochs)
        self.lambda_reg = lambda_reg

    def train(self, X_train, y_train):
        self.w = np.zeros((self.n_class, X_train.shape[1]))
        accuracies = []

        for epoch in range(self.epochs):
            scores = np.dot(X_train, self.w.T)
            y_hat = self.softmax(scores)

            loss = CrossEntropyLoss(y_train.values, y_hat)

            reg_loss = (self.lambda_reg / 2) * np.sum(self.w ** 2)
            total_loss = loss + reg_loss

            gradient = np.dot((y_hat - y_train.values).T, X_train) / X_train.shape[0]
            gradient += self.lambda_reg * self.w
            self.w -= self.lr * gradient

            y_pred_train = self.predict(X_train)
            accuracy = accuracy_score(np.argmax(y_train.values, axis=1), y_pred_train) * 100
            accuracies.append(float(accuracy))

            if epoch % 10 == 0:
                print(f"Epoch {epoch+1}/{self.epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%")

        return accuracies

model_improved = LogisticRegression2(n_class=y.shape[1], lr=0.1, epochs=100, lambda_reg=0.1)
accuracies_improved = model_improved.train(X_train, y_train)
accuracy_improved, f1_improved, conf_matrix_improved = model_improved.evaluate(X_test, y_test)

print("\nComparison of results:")
print(f"Baseline Model Accuracy: {model.evaluate(X_test, y_test)[0] * 100:.2f}%")
print(f"Improved Model Accuracy: {accuracy_improved * 100:.2f}%\n")

print(f"Baseline Model F1-Score: {model.evaluate(X_test, y_test)[1]:.2f}")
print(f"Improved Model F1-Score: {f1_improved:.2f}\n")

print("Baseline Confusion Matrix:")
print(model.evaluate(X_test, y_test)[2])

print("\nImproved Confusion Matrix:")
print(conf_matrix_improved)

plt.figure(figsize=(12, 6))
plt.plot(range(1, model.epochs + 1), model.train(X_train, y_train), label='Baseline Model', marker='o')
plt.plot(range(1, model_improved.epochs + 1), accuracies_improved, label='Improved Model', marker='x')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.title('Comparison of Accuracy over Epochs')
plt.legend()
plt.grid()
plt.show()